{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of SQuAD 1.0 dataset",
   "id": "dfbb285cbd617ffd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T19:10:42.030599Z",
     "start_time": "2024-04-24T19:10:37.815359Z"
    }
   },
   "source": [
    "from question_answering.paths import extractive_qa_paths\n",
    "from question_answering.utils import core_qa_utils\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karol\\miniconda3\\envs\\question_answering\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:11:16.530368Z",
     "start_time": "2024-04-24T19:11:16.477227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train_dataset, raw_test_dataset = core_qa_utils.load_datasets_from_json(\n",
    "    dataset_path=extractive_qa_paths.squad1_dataset_dir,\n",
    "    filenames=[\"original_train.json\", \"original_test.json\"],\n",
    ")"
   ],
   "id": "953c342afd23a354",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:11:33.313897Z",
     "start_time": "2024-04-24T19:11:33.297873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = raw_train_dataset.select(range(80000))\n",
    "val_dataset = raw_train_dataset.select(range(80000, 87599))\n",
    "test_dataset = raw_test_dataset"
   ],
   "id": "4c63b284d5640b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Maximum number of tokens in any sample across dataset",
   "id": "ad3e0b4ad621f2a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:12:28.572408Z",
     "start_time": "2024-04-24T19:12:28.564894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_sample(sample, tokenizer, max_tokens=None, padding=False):\n",
    "    question = sample[\"question\"].strip()\n",
    "    context = sample[\"context\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)"
   ],
   "id": "99034d24ca7e2dc7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "aebee1aaf133e589"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:14:51.318490Z",
     "start_time": "2024-04-24T19:14:48.345396Z"
    }
   },
   "cell_type": "code",
   "source": "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")",
   "id": "e7b3fa64f8614d79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbfc7585559a42b5aed80610684b2bb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karol\\miniconda3\\envs\\question_answering\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Karol\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50cf7925bfa4953a8fb6b4df981745f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8fc0776dfc8426186cf36414bc6b8c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec6e2f85f092463d98349cc1bce7290d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:17:35.891100Z",
     "start_time": "2024-04-24T19:16:59.221271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")"
   ],
   "id": "b008adeffddd207a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b68a19d7c8041a8b817560cfe43c480"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:21:49.330939Z",
     "start_time": "2024-04-24T19:21:44.381869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_uncased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_tokens = max(max_bert_uncased_train_tokens, max_bert_uncased_val_tokens, max_bert_uncased_test_tokens)\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_bert_uncased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_uncased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_uncased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_uncased_tokens}\")"
   ],
   "id": "a4bf100f3a7fe537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 870\n",
      "Max number of tokens in tokenized val dataset: 594\n",
      "Max number of tokens in tokenized test dataset: 819\n",
      "Max number of tokens overall: 870\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT cased",
   "id": "6177fdd882ec4093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:21.701134Z",
     "start_time": "2024-04-24T19:23:21.470010Z"
    }
   },
   "cell_type": "code",
   "source": "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")",
   "id": "7d9875298cdbc7be",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:26.404024Z",
     "start_time": "2024-04-24T19:23:21.812929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")"
   ],
   "id": "d632699c8b99adfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9723ae28d4b74ba5bd64a6ff19847700"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:33.048219Z",
     "start_time": "2024-04-24T19:23:28.354957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_tokens = max(max_bert_cased_train_tokens, max_bert_cased_val_tokens, max_bert_cased_test_tokens)\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_bert_cased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_cased_tokens}\")"
   ],
   "id": "68f4a541ec00d0d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 882\n",
      "Max number of tokens in tokenized val dataset: 614\n",
      "Max number of tokens in tokenized test dataset: 833\n",
      "Max number of tokens overall: 882\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ALBERT cased",
   "id": "a81ebd091218c79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:28:03.456649Z",
     "start_time": "2024-04-24T19:28:00.045633Z"
    }
   },
   "cell_type": "code",
   "source": "albert_cased_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")",
   "id": "e8e07b841367254",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2afe4cf1efe541e78e01b114f8d77a90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d2dbe07969147e4951a743abadd7f22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b338a3c427a9482c8d49204b6954eca9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f0d29a2c1714d9ca0bcca79af7d23b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:19.917838Z",
     "start_time": "2024-04-24T19:28:16.153232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")"
   ],
   "id": "91365b6d4c94932d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa6d9d1697f4d70ab875f3bf17596b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7599 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a41a0713c521471aabaa4e9224d97ebc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "723cb791e7754aaeaace07a170413822"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:24.705877Z",
     "start_time": "2024-04-24T19:29:19.918864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_albert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_tokens = max(max_albert_cased_train_tokens, max_albert_cased_val_tokens, max_albert_cased_test_tokens)\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_albert_cased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_albert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_albert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_albert_cased_tokens}\")"
   ],
   "id": "5f02fffde6568250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 978\n",
      "Max number of tokens in tokenized val dataset: 611\n",
      "Max number of tokens in tokenized test dataset: 824\n",
      "Max number of tokens overall: 978\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "585c7387fbf8c93f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
