{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of SQuAD 1.0 dataset",
   "id": "dfbb285cbd617ffd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.061159Z",
     "start_time": "2024-04-28T13:59:38.955681Z"
    }
   },
   "source": [
    "from question_answering.paths import extractive_qa_paths\n",
    "from question_answering.utils import core_qa_utils\n",
    "from transformers import AutoTokenizer"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.200156Z",
     "start_time": "2024-04-28T13:59:43.062164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train_dataset, raw_test_dataset = core_qa_utils.load_datasets_from_json(\n",
    "    dataset_path=extractive_qa_paths.squad1_dataset_dir,\n",
    "    filenames=[\"original_train.json\", \"original_test.json\"],\n",
    ")"
   ],
   "id": "953c342afd23a354",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.215671Z",
     "start_time": "2024-04-28T13:59:43.201161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = raw_train_dataset.select(range(80000))\n",
    "val_dataset = raw_train_dataset.select(range(80000, 87599))\n",
    "test_dataset = raw_test_dataset"
   ],
   "id": "4c63b284d5640b5",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Maximum number of tokens in any sample across dataset",
   "id": "ad3e0b4ad621f2a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:12:28.572408Z",
     "start_time": "2024-04-24T19:12:28.564894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_sample(sample, tokenizer, max_tokens=None, padding=False):\n",
    "    question = sample[\"question\"].strip()\n",
    "    context = sample[\"context\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)"
   ],
   "id": "99034d24ca7e2dc7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "aebee1aaf133e589"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:14:51.318490Z",
     "start_time": "2024-04-24T19:14:48.345396Z"
    }
   },
   "cell_type": "code",
   "source": "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")",
   "id": "e7b3fa64f8614d79",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:17:35.891100Z",
     "start_time": "2024-04-24T19:16:59.221271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")"
   ],
   "id": "b008adeffddd207a",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:21:49.330939Z",
     "start_time": "2024-04-24T19:21:44.381869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_uncased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_tokens = max(\n",
    "    max_bert_uncased_train_tokens,\n",
    "    max_bert_uncased_val_tokens,\n",
    "    max_bert_uncased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_bert_uncased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_uncased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_uncased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_uncased_tokens}\")"
   ],
   "id": "a4bf100f3a7fe537",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT cased",
   "id": "6177fdd882ec4093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:21.701134Z",
     "start_time": "2024-04-24T19:23:21.470010Z"
    }
   },
   "cell_type": "code",
   "source": "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")",
   "id": "7d9875298cdbc7be",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:26.404024Z",
     "start_time": "2024-04-24T19:23:21.812929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")"
   ],
   "id": "d632699c8b99adfa",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:33.048219Z",
     "start_time": "2024-04-24T19:23:28.354957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_tokens = max(\n",
    "    max_bert_cased_train_tokens, max_bert_cased_val_tokens, max_bert_cased_test_tokens\n",
    ")\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_bert_cased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_cased_tokens}\")"
   ],
   "id": "68f4a541ec00d0d8",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ALBERT cased",
   "id": "a81ebd091218c79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:28:03.456649Z",
     "start_time": "2024-04-24T19:28:00.045633Z"
    }
   },
   "cell_type": "code",
   "source": "albert_cased_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")",
   "id": "e8e07b841367254",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:19.917838Z",
     "start_time": "2024-04-24T19:28:16.153232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")"
   ],
   "id": "91365b6d4c94932d",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:24.705877Z",
     "start_time": "2024-04-24T19:29:19.918864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_albert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_tokens = max(\n",
    "    max_albert_cased_train_tokens,\n",
    "    max_albert_cased_val_tokens,\n",
    "    max_albert_cased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_albert_cased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_albert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_albert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_albert_cased_tokens}\")"
   ],
   "id": "5f02fffde6568250",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data samples structure",
   "id": "1b6ef3f5c601b143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train sample",
   "id": "b7b427253e3de359"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:46.129441Z",
     "start_time": "2024-04-28T13:59:46.123931Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample = val_dataset[0]",
   "id": "d9f233915ea3d82",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:52.941051Z",
     "start_time": "2024-04-28T13:59:52.929940Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample",
   "id": "5c8d91de5be6441d",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test sample",
   "id": "e39a9c3abb6a40e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T14:00:22.053479Z",
     "start_time": "2024-04-28T14:00:22.042973Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample = test_dataset[0]",
   "id": "5ec78074cd0a89f8",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T14:00:26.424168Z",
     "start_time": "2024-04-28T14:00:26.416670Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample",
   "id": "5eb7aa47ed68fedf",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokens histograms",
   "id": "2e6bf7fa447557e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:23:45.064030Z",
     "start_time": "2024-04-24T20:23:40.907696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_input_ids = tokenized_train_dataset[\"input_ids\"]\n",
    "below_maximum = []\n",
    "for sample_input_ids in train_input_ids:\n",
    "    if len(sample_input_ids) <= albert_cased_tokenizer.model_max_length:\n",
    "        below_maximum.append(sample_input_ids)\n",
    "\n",
    "len(below_maximum)"
   ],
   "id": "95ff7645eb3aab23",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:24:06.879875Z",
     "start_time": "2024-04-24T20:24:06.874792Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_input_ids)",
   "id": "e193f3db48ae31d1",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_token_distribution(\n",
    "    tokenized_bert_uncased_dataset,\n",
    "    tokenized_bert_cased_dataset,\n",
    "    tokenized_albert_cased_dataset,\n",
    "):\n",
    "    b\n",
    "\n",
    "    plt.subp"
   ],
   "id": "dcf0c8ab3857c8f4",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "a192fa6db0feaf81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
