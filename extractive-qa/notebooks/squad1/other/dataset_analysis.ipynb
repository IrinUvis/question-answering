{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of SQuAD 1.0 dataset",
   "id": "dfbb285cbd617ffd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.061159Z",
     "start_time": "2024-04-28T13:59:38.955681Z"
    }
   },
   "source": [
    "from question_answering.paths import extractive_qa_paths\n",
    "from question_answering.utils import core_qa_utils\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karol\\miniconda3\\envs\\question_answering\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.200156Z",
     "start_time": "2024-04-28T13:59:43.062164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train_dataset, raw_test_dataset = core_qa_utils.load_datasets_from_json(\n",
    "    dataset_path=extractive_qa_paths.squad1_dataset_dir,\n",
    "    filenames=[\"original_train.json\", \"original_test.json\"],\n",
    ")"
   ],
   "id": "953c342afd23a354",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:43.215671Z",
     "start_time": "2024-04-28T13:59:43.201161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = raw_train_dataset.select(range(80000))\n",
    "val_dataset = raw_train_dataset.select(range(80000, 87599))\n",
    "test_dataset = raw_test_dataset"
   ],
   "id": "4c63b284d5640b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Maximum number of tokens in any sample across dataset",
   "id": "ad3e0b4ad621f2a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:12:28.572408Z",
     "start_time": "2024-04-24T19:12:28.564894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_sample(sample, tokenizer, max_tokens=None, padding=False):\n",
    "    question = sample[\"question\"].strip()\n",
    "    context = sample[\"context\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)"
   ],
   "id": "99034d24ca7e2dc7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "aebee1aaf133e589"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:14:51.318490Z",
     "start_time": "2024-04-24T19:14:48.345396Z"
    }
   },
   "cell_type": "code",
   "source": "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")",
   "id": "e7b3fa64f8614d79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbfc7585559a42b5aed80610684b2bb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karol\\miniconda3\\envs\\question_answering\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Karol\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50cf7925bfa4953a8fb6b4df981745f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8fc0776dfc8426186cf36414bc6b8c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec6e2f85f092463d98349cc1bce7290d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:17:35.891100Z",
     "start_time": "2024-04-24T19:16:59.221271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")"
   ],
   "id": "b008adeffddd207a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b68a19d7c8041a8b817560cfe43c480"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:21:49.330939Z",
     "start_time": "2024-04-24T19:21:44.381869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_uncased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_tokens = max(\n",
    "    max_bert_uncased_train_tokens,\n",
    "    max_bert_uncased_val_tokens,\n",
    "    max_bert_uncased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_bert_uncased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_uncased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_uncased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_uncased_tokens}\")"
   ],
   "id": "a4bf100f3a7fe537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 870\n",
      "Max number of tokens in tokenized val dataset: 594\n",
      "Max number of tokens in tokenized test dataset: 819\n",
      "Max number of tokens overall: 870\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT cased",
   "id": "6177fdd882ec4093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:21.701134Z",
     "start_time": "2024-04-24T19:23:21.470010Z"
    }
   },
   "cell_type": "code",
   "source": "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")",
   "id": "7d9875298cdbc7be",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:26.404024Z",
     "start_time": "2024-04-24T19:23:21.812929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")"
   ],
   "id": "d632699c8b99adfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9723ae28d4b74ba5bd64a6ff19847700"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:23:33.048219Z",
     "start_time": "2024-04-24T19:23:28.354957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_tokens = max(\n",
    "    max_bert_cased_train_tokens, max_bert_cased_val_tokens, max_bert_cased_test_tokens\n",
    ")\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_bert_cased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_cased_tokens}\")"
   ],
   "id": "68f4a541ec00d0d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 882\n",
      "Max number of tokens in tokenized val dataset: 614\n",
      "Max number of tokens in tokenized test dataset: 833\n",
      "Max number of tokens overall: 882\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ALBERT cased",
   "id": "a81ebd091218c79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:28:03.456649Z",
     "start_time": "2024-04-24T19:28:00.045633Z"
    }
   },
   "cell_type": "code",
   "source": "albert_cased_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")",
   "id": "e8e07b841367254",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2afe4cf1efe541e78e01b114f8d77a90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d2dbe07969147e4951a743abadd7f22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b338a3c427a9482c8d49204b6954eca9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f0d29a2c1714d9ca0bcca79af7d23b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:19.917838Z",
     "start_time": "2024-04-24T19:28:16.153232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")"
   ],
   "id": "91365b6d4c94932d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa6d9d1697f4d70ab875f3bf17596b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7599 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a41a0713c521471aabaa4e9224d97ebc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "723cb791e7754aaeaace07a170413822"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:29:24.705877Z",
     "start_time": "2024-04-24T19:29:19.918864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_albert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_tokens = max(\n",
    "    max_albert_cased_train_tokens,\n",
    "    max_albert_cased_val_tokens,\n",
    "    max_albert_cased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_albert_cased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_albert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_albert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_albert_cased_tokens}\")"
   ],
   "id": "5f02fffde6568250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 978\n",
      "Max number of tokens in tokenized val dataset: 611\n",
      "Max number of tokens in tokenized test dataset: 824\n",
      "Max number of tokens overall: 978\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data samples structure",
   "id": "1b6ef3f5c601b143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train sample",
   "id": "b7b427253e3de359"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:46.129441Z",
     "start_time": "2024-04-28T13:59:46.123931Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample = val_dataset[0]",
   "id": "d9f233915ea3d82",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:59:52.941051Z",
     "start_time": "2024-04-28T13:59:52.929940Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample",
   "id": "5c8d91de5be6441d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57301a88b2c2fd1400568878',\n",
       " 'title': 'Liberia',\n",
       " 'context': 'The Americo-Liberian settlers did not identify with the indigenous peoples they encountered, especially those in communities of the more isolated \"bush.\" They knew nothing of their cultures, languages or animist religion. Encounters with tribal Africans in the bush often developed as violent confrontations. The colonial settlements were raided by the Kru and Grebo people from their inland chiefdoms. Because of feeling set apart and superior by their culture and education to the indigenous peoples, the Americo-Liberians developed as a small elite that held on to political power. It excluded the indigenous tribesmen from birthright citizenship in their own lands until 1904, in a repetition of the United States\\' treatment of Native Americans. Because of the cultural gap between the groups and assumption of superiority of western culture, the Americo-Liberians envisioned creating a western-style state to which the tribesmen should assimilate. They encouraged religious organizations to set up missions and schools to educate the indigenous peoples.',\n",
       " 'question': 'What did Americo-liberians exclude tribes from?',\n",
       " 'answers': {'text': ['citizenship in their own lands'],\n",
       "  'answer_start': [638]},\n",
       " 'answer_text': ['citizenship in their own lands'],\n",
       " 'answer_start': [638]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test sample",
   "id": "e39a9c3abb6a40e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T14:00:22.053479Z",
     "start_time": "2024-04-28T14:00:22.042973Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample = test_dataset[0]",
   "id": "5ec78074cd0a89f8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T14:00:26.424168Z",
     "start_time": "2024-04-28T14:00:26.416670Z"
    }
   },
   "cell_type": "code",
   "source": "train_sample",
   "id": "5eb7aa47ed68fedf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]},\n",
       " 'answer_text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokens histograms",
   "id": "2e6bf7fa447557e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:23:45.064030Z",
     "start_time": "2024-04-24T20:23:40.907696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_input_ids = tokenized_train_dataset[\"input_ids\"]\n",
    "below_maximum = []\n",
    "for sample_input_ids in train_input_ids:\n",
    "    if len(sample_input_ids) <= albert_cased_tokenizer.model_max_length:\n",
    "        below_maximum.append(sample_input_ids)\n",
    "\n",
    "len(below_maximum)"
   ],
   "id": "95ff7645eb3aab23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:24:06.879875Z",
     "start_time": "2024-04-24T20:24:06.874792Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_input_ids)",
   "id": "e193f3db48ae31d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_token_distribution(\n",
    "    tokenized_bert_uncased_dataset,\n",
    "    tokenized_bert_cased_dataset,\n",
    "    tokenized_albert_cased_dataset,\n",
    "):\n",
    "    b\n",
    "\n",
    "    plt.subp"
   ],
   "id": "dcf0c8ab3857c8f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "a192fa6db0feaf81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
