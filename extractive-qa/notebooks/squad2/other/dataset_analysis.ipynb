{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of SQuAD 2.0 dataset",
   "id": "d43c94954dc0460f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:28.169286Z",
     "start_time": "2024-04-28T11:12:24.353418Z"
    }
   },
   "source": [
    "from question_answering.paths import extractive_qa_paths\n",
    "from question_answering.utils import core_qa_utils\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karol\\miniconda3\\envs\\question_answering\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:28.233335Z",
     "start_time": "2024-04-28T11:12:28.171241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train_dataset, raw_test_dataset = core_qa_utils.load_datasets_from_json(\n",
    "    dataset_path=extractive_qa_paths.squad2_dataset_dir,\n",
    "    filenames=[\"original_train.json\", \"original_test.json\"],\n",
    ")"
   ],
   "id": "739fb1ed462c1ca5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:28.248917Z",
     "start_time": "2024-04-28T11:12:28.234336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = raw_train_dataset.select(range(115000))\n",
    "val_dataset = raw_train_dataset.select(range(115000, 130319))\n",
    "test_dataset = raw_test_dataset"
   ],
   "id": "ed37389ce5169eba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Maximum number of tokens in any sample across dataset",
   "id": "41988b399da961d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:28.264382Z",
     "start_time": "2024-04-28T11:12:28.249870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_sample(sample, tokenizer, max_tokens=None, padding=False):\n",
    "    question = sample[\"question\"].strip()\n",
    "    context = sample[\"context\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)"
   ],
   "id": "4ad7d5af57713d0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT uncased",
   "id": "b8edb7ba8b4e5aca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:12:28.826191Z",
     "start_time": "2024-04-28T11:12:28.265379Z"
    }
   },
   "cell_type": "code",
   "source": "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")",
   "id": "8ffac0a1af790c63",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:13:38.009884Z",
     "start_time": "2024-04-28T11:12:28.827192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_uncased_tokenizer)\n",
    ")"
   ],
   "id": "2962ed94211fc042",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a313c22e8174c778cb8268e6ed8825b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10319 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc5e472934954d25bc66d79b734dd2cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be7976c0e814c219ebbba1e65f761a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:13:45.174890Z",
     "start_time": "2024-04-28T11:13:38.010889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_uncased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_uncased_tokens = max(\n",
    "    max_bert_uncased_train_tokens,\n",
    "    max_bert_uncased_val_tokens,\n",
    "    max_bert_uncased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_bert_uncased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_uncased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_uncased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_uncased_tokens}\")"
   ],
   "id": "1b09bb010e37c6a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 870\n",
      "Max number of tokens in tokenized val dataset: 866\n",
      "Max number of tokens in tokenized test dataset: 819\n",
      "Max number of tokens overall: 870\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BERT cased",
   "id": "7b5508129a2a1266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:13:45.409186Z",
     "start_time": "2024-04-28T11:13:45.175894Z"
    }
   },
   "cell_type": "code",
   "source": "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")",
   "id": "319490912739a6e7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:47.522217Z",
     "start_time": "2024-04-28T11:13:45.410421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, bert_cased_tokenizer)\n",
    ")"
   ],
   "id": "4727679867115932",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d74abf8b67cd4c1a980124f815448bd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10319 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "890e84ef8923447e973498e356c4afaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "983532c3056740d096aa22bac7806e39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:54.765958Z",
     "start_time": "2024-04-28T11:14:47.524223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_bert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_bert_cased_tokens = max(\n",
    "    max_bert_cased_train_tokens, max_bert_cased_val_tokens, max_bert_cased_test_tokens\n",
    ")\n",
    "\n",
    "print(f\"Max number of tokens in tokenized train dataset: {max_bert_cased_train_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_bert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_bert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_bert_cased_tokens}\")"
   ],
   "id": "bae1978efb441dda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 882\n",
      "Max number of tokens in tokenized val dataset: 878\n",
      "Max number of tokens in tokenized test dataset: 833\n",
      "Max number of tokens overall: 882\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ALBERT cased",
   "id": "f03408053f1414ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:14:54.985105Z",
     "start_time": "2024-04-28T11:14:54.766958Z"
    }
   },
   "cell_type": "code",
   "source": "albert_cased_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")",
   "id": "6c72a88c4b028f63",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:31.956105Z",
     "start_time": "2024-04-28T11:14:54.986132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    lambda row: tokenize_sample(row, albert_cased_tokenizer)\n",
    ")"
   ],
   "id": "832c35a375109617",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3982288577ca453da803f2b2cbb3bef1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10319 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6383472cf1e47c2bcb28601f0efeb1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5912d1fea0c49c59745c6387ef6b724"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:16:39.250503Z",
     "start_time": "2024-04-28T11:16:31.958108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_albert_cased_train_tokens = len(max(tokenized_train_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_val_tokens = len(max(tokenized_val_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_test_tokens = len(max(tokenized_test_dataset[\"input_ids\"], key=len))\n",
    "max_albert_cased_tokens = max(\n",
    "    max_albert_cased_train_tokens,\n",
    "    max_albert_cased_val_tokens,\n",
    "    max_albert_cased_test_tokens,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Max number of tokens in tokenized train dataset: {max_albert_cased_train_tokens}\"\n",
    ")\n",
    "print(f\"Max number of tokens in tokenized val dataset: {max_albert_cased_val_tokens}\")\n",
    "print(f\"Max number of tokens in tokenized test dataset: {max_albert_cased_test_tokens}\")\n",
    "print(f\"Max number of tokens overall: {max_albert_cased_tokens}\")"
   ],
   "id": "444c9e4b7f71c4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset: 978\n",
      "Max number of tokens in tokenized val dataset: 974\n",
      "Max number of tokens in tokenized test dataset: 824\n",
      "Max number of tokens overall: 978\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data samples structure",
   "id": "ac4639a6e267b144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train sample with answer",
   "id": "e35f515bf95c0bc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:17:48.016023Z",
     "start_time": "2024-04-28T11:17:48.005989Z"
    }
   },
   "cell_type": "code",
   "source": "sample_with_answer = val_dataset[0]",
   "id": "682d194088846bf6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:39:37.032529Z",
     "start_time": "2024-04-28T11:39:37.022010Z"
    }
   },
   "cell_type": "code",
   "source": "sample_with_answer",
   "id": "11a8849f76c43a39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57324e38e17f3d1400422826',\n",
       " 'title': 'Dwight_D._Eisenhower',\n",
       " 'context': 'Following the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis.',\n",
       " 'question': 'Aside from the US, what other countries had occupation zones in Germany?',\n",
       " 'answers': {'text': ['Britain, France and the Soviet Union'],\n",
       "  'answer_start': [238]},\n",
       " 'answer_text': ['Britain, France and the Soviet Union'],\n",
       " 'answer_start': [238]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train sample without answer",
   "id": "7e8f3404bb5eafa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:39:48.862406Z",
     "start_time": "2024-04-28T11:39:48.851016Z"
    }
   },
   "cell_type": "code",
   "source": "sample_without_answer = val_dataset[5]",
   "id": "e947bb27e8b2efae",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:39:49.128504Z",
     "start_time": "2024-04-28T11:39:49.116986Z"
    }
   },
   "cell_type": "code",
   "source": "sample_without_answer",
   "id": "d388f11bb8f61729",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a711df70efcfe001a8afd4f',\n",
       " 'title': 'Pesticide',\n",
       " 'context': 'Some evidence shows that alternatives to pesticides can be equally effective as the use of chemicals. For example, Sweden has halved its use of pesticides with hardly any reduction in crops.[unreliable source?] In Indonesia, farmers have reduced pesticide use on rice fields by 65% and experienced a 15% crop increase.[unreliable source?] A study of Maize fields in northern Florida found that the application of composted yard waste with high carbon to nitrogen ratio to agricultural fields was highly effective at reducing the population of plant-parasitic nematodes and increasing crop yield, with yield increases ranging from 10% to 212%; the observed effects were long-term, often not appearing until the third season of the study.',\n",
       " 'question': 'What happened after Indonesia cut its pesticide use in half?',\n",
       " 'answers': {'text': [], 'answer_start': []},\n",
       " 'answer_text': [],\n",
       " 'answer_start': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test sample with answer",
   "id": "9362533aab9057f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:44:26.688330Z",
     "start_time": "2024-04-28T11:44:26.678325Z"
    }
   },
   "cell_type": "code",
   "source": "test_sample_with_answer = test_dataset[1]",
   "id": "30510ee129787f6",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:44:27.826855Z",
     "start_time": "2024-04-28T11:44:27.819852Z"
    }
   },
   "cell_type": "code",
   "source": "test_sample_with_answer",
   "id": "7d4c42c30d1d1cef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5ad26a5fd7d075001a42931b',\n",
       " 'title': 'Force',\n",
       " 'context': 'Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force exactly balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.',\n",
       " 'question': 'What increases or decreases in response to static friction?',\n",
       " 'answers': {'text': [], 'answer_start': []},\n",
       " 'answer_text': [],\n",
       " 'answer_start': []}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test sample without answer",
   "id": "4a200f0904f2324f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:44:32.558464Z",
     "start_time": "2024-04-28T11:44:32.544376Z"
    }
   },
   "cell_type": "code",
   "source": "test_sample_without_answer = test_dataset[0]",
   "id": "eb237dff9ae08c5d",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:44:32.856261Z",
     "start_time": "2024-04-28T11:44:32.852266Z"
    }
   },
   "cell_type": "code",
   "source": "test_sample_without_answer",
   "id": "84fad134210e1c3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733ea04d058e614000b6595',\n",
       " 'title': 'French_and_Indian_War',\n",
       " 'context': \"In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier, but where Céloron had limited the record of French claims to the burial of lead plates, Marin constructed and garrisoned forts. He first constructed Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore. He had a road built to the headwaters of LeBoeuf Creek. Marin constructed a second fort at Fort Le Boeuf (present-day Waterford, Pennsylvania), designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson, a chief of the Mingo, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French (whom he accused of killing and eating his father). Traveling to Fort Le Boeuf, he threatened the French with military action, which Marin contemptuously dismissed.\",\n",
       " 'question': 'Where did Marin build first fort?',\n",
       " 'answers': {'text': ['Fort Presque Isle (near present-day Erie, Pennsylvania',\n",
       "   'Fort Presque Isle',\n",
       "   'near present-day Erie, Pennsylvania',\n",
       "   'Fort Presque Isle',\n",
       "   'near present-day Erie, Pennsylvania'],\n",
       "  'answer_start': [425, 425, 444, 425, 444]},\n",
       " 'answer_text': ['Fort Presque Isle (near present-day Erie, Pennsylvania',\n",
       "  'Fort Presque Isle',\n",
       "  'near present-day Erie, Pennsylvania',\n",
       "  'Fort Presque Isle',\n",
       "  'near present-day Erie, Pennsylvania'],\n",
       " 'answer_start': [425, 425, 444, 425, 444]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1cb01c23a8887700"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
